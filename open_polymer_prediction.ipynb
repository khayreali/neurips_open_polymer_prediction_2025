{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a484b4f1",
   "metadata": {
    "papermill": {
     "duration": 0.006567,
     "end_time": "2025-09-15T18:59:59.145047",
     "exception": false,
     "start_time": "2025-09-15T18:59:59.138480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NEURIPS - Open Polymer Prediction 2025\n",
    "#### Majd Shammout and Khayre Ali (CUDAWOULDASHOULDA)\n",
    "\n",
    "#### Project Overview\n",
    "This notebook works to predict five target variables related to physical properties of polymers. \n",
    "\n",
    "1) Glass Transition Temperature (`Tg`)\n",
    "2) Fractional Free Volume (`FFV`)\n",
    "3) Crystallization Temperature (`Tc`)\n",
    "4) Density\n",
    "5) Radius of Gyration (`Rg`)\n",
    "\n",
    "#### Methodology\n",
    "Our team leveraged a stacked ensemble of tree-based models, XGBoost & RandomForest, and a pre-trained GNN. The tree-based models are trained on chemical features from libraries like RDKit and molecular fingerprints. The GNN learns representations from the molecular graph structures - particularly relationships that the 2D descriptors would not capture.\n",
    "\n",
    "The final predication is generated by a meta-model that learns the best way to combine the outputs from the tree-based and GNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff94a25",
   "metadata": {
    "papermill": {
     "duration": 0.003848,
     "end_time": "2025-09-15T18:59:59.153160",
     "exception": false,
     "start_time": "2025-09-15T18:59:59.149312",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports/installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ee3e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T18:59:59.162325Z",
     "iopub.status.busy": "2025-09-15T18:59:59.162030Z",
     "iopub.status.idle": "2025-09-15T19:00:33.607016Z",
     "shell.execute_reply": "2025-09-15T19:00:33.606045Z"
    },
    "papermill": {
     "duration": 34.451733,
     "end_time": "2025-09-15T19:00:33.608851",
     "exception": false,
     "start_time": "2025-09-15T18:59:59.157118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl -q\n",
    "!pip install /kaggle/input/torch-geometric-2-6-1/torch_geometric-2.6.1-py3-none-any.whl -q\n",
    "!pip install mordred --no-index --find-links=file:///kaggle/input/mordred-1-2-0-py3-none-any/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, MACCSkeys, rdMolDescriptors, rdmolops\n",
    "from rdkit.Chem.rdFingerprintGenerator import GetMorganGenerator\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "import networkx as nx\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0bcfc",
   "metadata": {
    "papermill": {
     "duration": 0.003265,
     "end_time": "2025-09-15T19:00:33.616214",
     "exception": false,
     "start_time": "2025-09-15T19:00:33.612949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521a61db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:00:33.624822Z",
     "iopub.status.busy": "2025-09-15T19:00:33.624256Z",
     "iopub.status.idle": "2025-09-15T19:00:33.682786Z",
     "shell.execute_reply": "2025-09-15T19:00:33.681788Z"
    },
    "papermill": {
     "duration": 0.064369,
     "end_time": "2025-09-15T19:00:33.684170",
     "exception": false,
     "start_time": "2025-09-15T19:00:33.619801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples 7973\n",
      "Training samples 3\n",
      "tg values: 511, missing tg values: 7462\n",
      "ffv values: 7030, missing ffv values: 943\n",
      "tc values: 737, missing tc values: 7236\n",
      "density values: 613, missing density values: 7360\n",
      "rg values: 613, missing rg values: 7359\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/neurips-open-polymer-prediction-2025/test.csv\")\n",
    "print(f\"Training samples {len(train)}\")\n",
    "print(f\"Training samples {len(test)}\")\n",
    "print(f\"tg values: {len(train.loc[train['Tg'].notna()])}, missing tg values: {len(train.loc[train['Tg'].isna()])}\")\n",
    "print(f\"ffv values: {len(train.loc[train['FFV'].notna()])}, missing ffv values: {len(train.loc[train['FFV'].isna()])}\")\n",
    "print(f\"tc values: {len(train.loc[train['Tc'].notna()])}, missing tc values: {len(train.loc[train['Tc'].isna()])}\")\n",
    "print(f\"density values: {len(train.loc[train['Density'].notna()])}, missing density values: {len(train.loc[train['Density'].isna()])}\")\n",
    "print(f\"rg values: {len(train.loc[train['Density'].notna()])}, missing rg values: {len(train.loc[train['Rg'].isna()])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a347e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:00:33.693691Z",
     "iopub.status.busy": "2025-09-15T19:00:33.693403Z",
     "iopub.status.idle": "2025-09-15T19:00:51.527527Z",
     "shell.execute_reply": "2025-09-15T19:00:51.526494Z"
    },
    "papermill": {
     "duration": 17.840899,
     "end_time": "2025-09-15T19:00:51.529068",
     "exception": false,
     "start_time": "2025-09-15T19:00:33.688169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading external datasets...\n",
      "Processing 874 Tc samples\n",
      "Valid samples: 874/874\n",
      "Tc: Added 129 samples, 129 unique SMILES\n",
      "Tc data loaded successfully\n",
      "Processing 194 Tg samples\n",
      "Valid samples: 194/194\n",
      "Tg: Added 90 samples, 57 unique SMILES\n",
      "Processing 194 Density samples\n",
      "Valid samples: 181/194\n",
      "Density: Added 61 samples, 0 unique SMILES\n",
      "Tg/Density data loaded successfully\n",
      "Processing 7284 Tg samples\n",
      "Valid samples: 7284/7284\n",
      "Tg: Added 7125 samples, 1918 unique SMILES\n",
      "TgSS data loaded successfully\n",
      "Processing 662 Tg samples\n",
      "Valid samples: 662/662\n",
      "Tg: Added 70 samples, 62 unique SMILES\n",
      "JCIM data loaded successfully\n",
      "Processing 501 Tg samples\n",
      "Valid samples: 501/501\n",
      "Tg: Added 499 samples, 499 unique SMILES\n",
      "Excel Tg data loaded successfully\n",
      "Processing 786 Density samples\n",
      "Valid samples: 780/786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[19:00:49] SMILES Parse Error: syntax error while parsing: *O[Si](*)([R])[R]\n",
      "[19:00:49] SMILES Parse Error: check for mistakes around position 12:\n",
      "[19:00:49] *O[Si](*)([R])[R]\n",
      "[19:00:49] ~~~~~~~~~~~^\n",
      "[19:00:49] SMILES Parse Error: Failed parsing SMILES '*O[Si](*)([R])[R]' for input: '*O[Si](*)([R])[R]'\n",
      "[19:00:49] SMILES Parse Error: syntax error while parsing: *NC(=O)c4ccc3c(=O)n(c2ccc([R]c1ccc(*)cc1)cc2)c(=O)c3c4\n",
      "[19:00:49] SMILES Parse Error: check for mistakes around position 28:\n",
      "[19:00:49] c4ccc3c(=O)n(c2ccc([R]c1ccc(*)cc1)cc2)c(=\n",
      "[19:00:49] ~~~~~~~~~~~~~~~~~~~~^\n",
      "[19:00:49] SMILES Parse Error: Failed parsing SMILES '*NC(=O)c4ccc3c(=O)n(c2ccc([R]c1ccc(*)cc1)cc2)c(=O)c3c4' for input: '*NC(=O)c4ccc3c(=O)n(c2ccc([R]c1ccc(*)cc1)cc2)c(=O)c3c4'\n",
      "[19:00:49] SMILES Parse Error: syntax error while parsing: O=C=N[R1]N=C=O.O[R2]O.O[R3]O\n",
      "[19:00:49] SMILES Parse Error: check for mistakes around position 7:\n",
      "[19:00:49] O=C=N[R1]N=C=O.O[R2]O.O[R3]O\n",
      "[19:00:49] ~~~~~~^\n",
      "[19:00:49] SMILES Parse Error: Failed parsing SMILES 'O=C=N[R1]N=C=O.O[R2]O.O[R3]O' for input: 'O=C=N[R1]N=C=O.O[R2]O.O[R3]O'\n",
      "[19:00:49] SMILES Parse Error: syntax error while parsing: *CN([R'])Cc2cc([R]c1cc(*)c(O)c(CN([R'])C*)c1)cc(*)c2O\n",
      "[19:00:49] SMILES Parse Error: check for mistakes around position 6:\n",
      "[19:00:49] *CN([R'])Cc2cc([R]c1cc(*)c(O)c(CN([R'])C*\n",
      "[19:00:49] ~~~~~^\n",
      "[19:00:49] SMILES Parse Error: Failed parsing SMILES '*CN([R'])Cc2cc([R]c1cc(*)c(O)c(CN([R'])C*)c1)cc(*)c2O' for input: '*CN([R'])Cc2cc([R]c1cc(*)c(O)c(CN([R'])C*)c1)cc(*)c2O'\n",
      "[19:00:49] SMILES Parse Error: syntax error while parsing: *C(F)(F)CC(F)([R])C(*)(F)F\n",
      "[19:00:49] SMILES Parse Error: check for mistakes around position 16:\n",
      "[19:00:49] *C(F)(F)CC(F)([R])C(*)(F)F\n",
      "[19:00:49] ~~~~~~~~~~~~~~~^\n",
      "[19:00:49] SMILES Parse Error: Failed parsing SMILES '*C(F)(F)CC(F)([R])C(*)(F)F' for input: '*C(F)(F)CC(F)([R])C(*)(F)F'\n",
      "[19:00:49] SMILES Parse Error: syntax error while parsing: *OC2OC(CO[R])C(OC1OC(CO[R])C(*)C(O[R])C1O[R])C(O[R])C2O[R]\n",
      "[19:00:49] SMILES Parse Error: check for mistakes around position 11:\n",
      "[19:00:49] *OC2OC(CO[R])C(OC1OC(CO[R])C(*)C(O[R])C1O\n",
      "[19:00:49] ~~~~~~~~~~^\n",
      "[19:00:49] SMILES Parse Error: Failed parsing SMILES '*OC2OC(CO[R])C(OC1OC(CO[R])C(*)C(O[R])C1O[R])C(O[R])C2O[R]' for input: '*OC2OC(CO[R])C(OC1OC(CO[R])C(*)C(O[R])C1O[R])C(O[R])C2O[R]'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density: Added 623 samples, 472 unique SMILES\n",
      "Density data loaded successfully\n",
      "Processing 862 FFV samples\n",
      "Valid samples: 862/862\n",
      "FFV: Added 862 samples, 272 unique SMILES\n",
      "Dataset 4 (FFV) loaded successfully\n",
      "\n",
      "Data integration summary:\n",
      "Original samples: 7973\n",
      "Extended samples: 11382\n",
      "Total gain: 3409 samples\n",
      "Tg: 8,295 samples (gain: 7784)\n",
      "FFV: 7,892 samples (gain: 862)\n",
      "Tc: 866 samples (gain: 129)\n",
      "Density: 1,297 samples (gain: 684)\n",
      "Rg: 614 samples (gain: 0)\n",
      "\n",
      "Data integration complete\n"
     ]
    }
   ],
   "source": [
    "def clean_and_validate_smiles_vectorized(smiles_series):\n",
    "    def clean_single(smiles):\n",
    "        if not isinstance(smiles, str) or len(smiles) == 0:\n",
    "            return None\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    \n",
    "    return smiles_series.apply(clean_single)\n",
    "\n",
    "print(\"\\nLoading external datasets...\")\n",
    "\n",
    "def add_extra_data_clean(df_train, df_extra, target):\n",
    "    n_samples_before = len(df_train[df_train[target].notnull()])\n",
    "    \n",
    "    print(f\"Processing {len(df_extra)} {target} samples\")\n",
    "    \n",
    "    df_extra['SMILES'] = clean_and_validate_smiles_vectorized(df_extra['SMILES'])\n",
    "    \n",
    "    before_filter = len(df_extra)\n",
    "    df_extra = df_extra[df_extra['SMILES'].notnull()]\n",
    "    df_extra = df_extra.dropna(subset=[target])\n",
    "    after_filter = len(df_extra)\n",
    "    \n",
    "    print(f\"Valid samples: {after_filter}/{before_filter}\")\n",
    "    \n",
    "    if len(df_extra) == 0:\n",
    "        print(f\"No valid data for {target}\")\n",
    "        return df_train\n",
    "    \n",
    "    df_extra = df_extra.groupby('SMILES', as_index=False)[target].mean()\n",
    "    \n",
    "    cross_smiles = set(df_extra['SMILES']) & set(df_train['SMILES'])\n",
    "    unique_smiles_extra = set(df_extra['SMILES']) - set(df_train['SMILES'])\n",
    "\n",
    "    for smile in df_train[df_train[target].isnull()]['SMILES'].tolist():\n",
    "        if smile in cross_smiles:\n",
    "            df_train.loc[df_train['SMILES']==smile, target] = \\\n",
    "                df_extra[df_extra['SMILES']==smile][target].values[0]\n",
    "    \n",
    "    extra_to_add = df_extra[df_extra['SMILES'].isin(unique_smiles_extra)].copy()\n",
    "    if len(extra_to_add) > 0:\n",
    "        TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "        for col in TARGETS:\n",
    "            if col not in extra_to_add.columns:\n",
    "                extra_to_add[col] = np.nan\n",
    "        extra_to_add = extra_to_add[['SMILES'] + TARGETS]\n",
    "        df_train = pd.concat([df_train, extra_to_add], axis=0, ignore_index=True)\n",
    "\n",
    "    n_samples_after = len(df_train[df_train[target].notnull()])\n",
    "    print(f'{target}: Added {n_samples_after-n_samples_before} samples, {len(unique_smiles_extra)} unique SMILES')\n",
    "    return df_train\n",
    "\n",
    "train_extended = train[['SMILES', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']].copy()\n",
    "\n",
    "try:\n",
    "    tc_data = pd.read_csv('/kaggle/input/external-polymer-data/Tc_SMILES.csv')\n",
    "    tc_data = tc_data.rename(columns={'TC_mean': 'Tc'})\n",
    "    train_extended = add_extra_data_clean(train_extended, tc_data, 'Tc')\n",
    "    print(\"Tc data loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Tc data: {str(e)[:100]}\")\n",
    "\n",
    "try:\n",
    "    tg_density_data = pd.read_csv('/kaggle/input/polymer-tg-density-excerpt/tg_density.csv')\n",
    "    train_extended = add_extra_data_clean(train_extended, tg_density_data, 'Tg')\n",
    "    train_extended = add_extra_data_clean(train_extended, tg_density_data, 'Density')\n",
    "    print(\"Tg/Density data loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Tg/Density data: {str(e)[:100]}\")\n",
    "\n",
    "try:\n",
    "    tgss_data = pd.read_csv('/kaggle/input/external-polymer-data/TgSS_enriched_cleaned.csv')\n",
    "    if 'Tg' in tgss_data.columns:\n",
    "        train_extended = add_extra_data_clean(train_extended, tgss_data[['SMILES', 'Tg']], 'Tg')\n",
    "        print(\"TgSS data loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load TgSS data: {str(e)[:100]}\")\n",
    "\n",
    "try:\n",
    "    jcim_data = pd.read_csv('/kaggle/input/external-polymer-data/JCIM_sup_bigsmiles.csv')\n",
    "    jcim_data = jcim_data[['SMILES', 'Tg (C)']].rename(columns={'Tg (C)': 'Tg'})\n",
    "    train_extended = add_extra_data_clean(train_extended, jcim_data, 'Tg')\n",
    "    print(\"JCIM data loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load JCIM data: {str(e)[:100]}\")\n",
    "\n",
    "try:\n",
    "    tg_xlsx = pd.read_excel('/kaggle/input/external-polymer-data/data_tg3.xlsx', sheet_name='Лист1')\n",
    "    tg_xlsx = tg_xlsx.rename(columns={'Tg [K]': 'Tg'})\n",
    "    tg_xlsx['Tg'] = tg_xlsx['Tg'] - 273.15\n",
    "    train_extended = add_extra_data_clean(train_extended, tg_xlsx[['SMILES', 'Tg']], 'Tg')\n",
    "    print(\"Excel Tg data loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Excel Tg data: {str(e)[:100]}\")\n",
    "\n",
    "try:\n",
    "    density_data = pd.read_excel('/kaggle/input/external-polymer-data/data_dnst1.xlsx', sheet_name='MatrixRaw')\n",
    "    density_data = density_data.rename(columns={'density(g/cm3)': 'Density'})\n",
    "    density_data = density_data[['SMILES', 'Density']]\n",
    "    density_data = density_data.query('SMILES.notnull() and Density.notnull()')\n",
    "    density_data['Density'] = pd.to_numeric(density_data['Density'], errors='coerce')\n",
    "    density_data = density_data.dropna(subset=['Density'])\n",
    "    density_data['Density'] = density_data['Density'].astype(float) - 0.118\n",
    "    train_extended = add_extra_data_clean(train_extended, density_data, 'Density')\n",
    "    print(\"Density data loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Density data: {str(e)[:100]}\")\n",
    "\n",
    "try:\n",
    "    dataset4 = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/train_supplement/dataset4.csv')\n",
    "    if 'FFV' in dataset4.columns:\n",
    "        train_extended = add_extra_data_clean(train_extended, dataset4[['SMILES', 'FFV']], 'FFV')\n",
    "        print(\"Dataset 4 (FFV) loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load Dataset 4: {str(e)[:100]}\")\n",
    "\n",
    "print(f\"\\nData integration summary:\")\n",
    "print(f\"Original samples: {len(train)}\")\n",
    "print(f\"Extended samples: {len(train_extended)}\")\n",
    "print(f\"Total gain: {len(train_extended) - len(train)} samples\")\n",
    "\n",
    "for target in ['Tg', 'FFV', 'Tc', 'Density', 'Rg']:\n",
    "    count = train_extended[target].notna().sum()\n",
    "    original_count = train[target].notna().sum() if target in train.columns else 0\n",
    "    gain = count - original_count\n",
    "    print(f\"{target}: {count:,} samples (gain: {gain})\")\n",
    "\n",
    "train = train_extended\n",
    "print(\"\\nData integration complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a8e80d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:00:51.539444Z",
     "iopub.status.busy": "2025-09-15T19:00:51.538929Z",
     "iopub.status.idle": "2025-09-15T19:00:56.872618Z",
     "shell.execute_reply": "2025-09-15T19:00:56.871543Z"
    },
    "papermill": {
     "duration": 5.340259,
     "end_time": "2025-09-15T19:00:56.874224",
     "exception": false,
     "start_time": "2025-09-15T19:00:51.533965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problematic smiles to remove from train dataset: 0\n",
      "Problematic smiles to remove from test dataset: 0\n"
     ]
    }
   ],
   "source": [
    "train['SMILES'] = clean_and_validate_smiles_vectorized(train['SMILES'])\n",
    "test['SMILES'] = clean_and_validate_smiles_vectorized(test['SMILES'])\n",
    "\n",
    "print(f\"Problematic smiles to remove from train dataset: {len(train.loc[train['SMILES'].isna()])}\")\n",
    "print(f\"Problematic smiles to remove from test dataset: {len(test.loc[test['SMILES'].isna()])}\")\n",
    "\n",
    "train = train.loc[train['SMILES'].notnull()]\n",
    "test = test.loc[test['SMILES'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e162e",
   "metadata": {
    "papermill": {
     "duration": 0.004754,
     "end_time": "2025-09-15T19:00:56.883837",
     "exception": false,
     "start_time": "2025-09-15T19:00:56.879083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a14a41d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:00:56.894210Z",
     "iopub.status.busy": "2025-09-15T19:00:56.893808Z",
     "iopub.status.idle": "2025-09-15T19:00:56.910606Z",
     "shell.execute_reply": "2025-09-15T19:00:56.909761Z"
    },
    "papermill": {
     "duration": 0.023332,
     "end_time": "2025-09-15T19:00:56.911873",
     "exception": false,
     "start_time": "2025-09-15T19:00:56.888541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "required_descriptors = {'graph_diameter', 'num_cycles', 'avg_shortest_path', 'MolWt', 'LogP', 'TPSA', 'RotatableBonds', 'NumAtoms'}\n",
    "\n",
    "filters = {\n",
    "    'Tg': list(set(['NumRotatableBonds', 'NumAromaticRings', 'NumHDonors', 'NumHAcceptors', 'FractionCsp3', 'BertzCT', 'LabuteASA', 'MolWt', 'TPSA', 'MolLogP', 'FractionCsp2', 'HeavyAtomCount', 'NumAromaticCarbocycles', 'NumSaturatedRings', 'ExactMolWt', 'Chi0', 'Chi1', 'Kappa2', 'NumHeteroatoms', 'fr_ester', 'fr_ether', 'fr_amide', 'NumAliphaticRings', 'MaxAbsPartialCharge', 'MinAbsPartialCharge', 'MolMR', 'HallKierAlpha', 'Chi0v', 'Chi1v', 'NumRadicalElectrons', 'NumValenceElectrons', 'fr_benzene', 'fr_C_O', 'fr_C_O_noCOO', 'fr_COO', 'fr_COO2', 'fr_ArN', 'fr_Ar_N', 'fr_Ar_OH', 'BalabanJ', 'Ipc', 'Kappa1', 'Kappa3']).union(required_descriptors)),\n",
    "    \n",
    "    'Tc': list(set(['FractionCsp3', 'FractionCsp2', 'NumRotatableBonds', 'NumAromaticRings', 'NumAliphaticRings', 'RingCount', 'MolWt', 'HeavyAtomCount', 'NumHBondDonors', 'NumHBondAcceptors', 'TPSA', 'MolLogP', 'BalabanJ', 'BertzCT', 'HallKierAlpha', 'Kappa1', 'Kappa2', 'Kappa3', 'Phi', 'Chi0', 'Chi1', 'Chi2', 'Chi3', 'Chi4', 'EState_VSA1', 'EState_VSA2', 'EState_VSA3', 'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3', 'MinPartialCharge', 'MaxPartialCharge', 'MinAbsPartialCharge', 'MaxAbsPartialCharge', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_ester', 'fr_ether', 'fr_amide', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_benzene', 'fr_para_hydroxylation', 'NumSaturatedRings', 'NumSaturatedCarbocycles', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'ExactMolWt', 'MolMR', 'LabuteASA', 'Chi0v', 'Chi1v', 'SMR_VSA1', 'SMR_VSA2', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'Ipc', 'MinEStateIndex', 'MaxEStateIndex', 'MinAbsEStateIndex', 'MaxAbsEStateIndex', 'qed', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3']).union(required_descriptors)),\n",
    "    \n",
    "    'FFV': list(set(['MolWt', 'ExactMolWt', 'HeavyAtomCount', 'NumRotatableBonds', 'FractionCsp3', 'FractionCsp2', 'FractionCsp', 'LabuteASA', 'TPSA', 'MolLogP', 'MolMR', 'HallKierAlpha', 'Kappa1', 'Kappa2', 'Kappa3', 'Phi', 'Chi0', 'Chi1', 'Chi2', 'Chi3', 'Chi4', 'Chi0v', 'Chi1v', 'Chi2v', 'Chi3v', 'Chi4v', 'BalabanJ', 'BertzCT', 'Ipc', 'MinPartialCharge', 'MaxPartialCharge', 'MinAbsPartialCharge', 'MaxAbsPartialCharge', 'MinEStateIndex', 'MaxEStateIndex', 'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'SMR_VSA1', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SlogP_VSA1', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'EState_VSA1', 'EState_VSA2', 'EState_VSA3', 'VSA_EState1', 'VSA_EState2', 'VSA_EState3', 'NumHBondDonors', 'NumHBondAcceptors', 'NumAromaticRings', 'NumSaturatedRings', 'NumAliphaticRings', 'RingCount', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_ester', 'fr_ether', 'fr_Ar_OH', 'fr_phenol', 'fr_benzene', 'fr_Ar_N', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_quatN', 'fr_halogen', 'fr_Al_OH', 'fr_methoxy', 'NumHeteroatoms', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'qed', 'fr_sulfide', 'fr_sulfone', 'fr_sulfonamd', 'fr_nitrile', 'fr_Imine', 'PEOE_VSA9', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SMR_VSA10', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'EState_VSA10', 'EState_VSA11', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'VSA_EState10', 'NHOHCount', 'NOCount', 'NumRadicalElectrons', 'NumValenceElectrons']).union(required_descriptors)),\n",
    "    \n",
    "    'Density': list(set(['MolWt', 'ExactMolWt', 'HeavyAtomCount', 'NumRotatableBonds', 'FractionCsp3', 'FractionCsp2', 'FractionCsp', 'LabuteASA', 'TPSA', 'MolLogP', 'MolMR', 'HallKierAlpha', 'Kappa1', 'Kappa2', 'Kappa3', 'Phi', 'Chi0', 'Chi1', 'Chi2', 'Chi3', 'Chi4', 'Chi0v', 'Chi1v', 'Chi2v', 'Chi3v', 'Chi4v', 'BalabanJ', 'BertzCT', 'Ipc', 'MinPartialCharge', 'MaxPartialCharge', 'MinAbsPartialCharge', 'MaxAbsPartialCharge', 'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'SMR_VSA1', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SlogP_VSA1', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'EState_VSA1', 'EState_VSA2', 'EState_VSA3', 'VSA_EState1', 'VSA_EState2', 'VSA_EState3', 'NumHBondDonors', 'NumHBondAcceptors', 'NumAromaticRings', 'NumSaturatedRings', 'NumAliphaticRings', 'RingCount', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'AvgIpc', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_ester', 'fr_ether', 'fr_amide', 'fr_urea', 'fr_Ar_OH', 'fr_phenol', 'fr_benzene', 'fr_Ar_N', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_halogen', 'fr_Al_OH', 'fr_methoxy', 'fr_sulfide', 'fr_sulfone', 'NumHeteroatoms', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'qed', 'Chi0n', 'Chi1n', 'Chi2n', 'Chi3n', 'Chi4n', 'MinEStateIndex', 'MaxEStateIndex', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'PEOE_VSA10', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SMR_VSA9', 'SMR_VSA10', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA9', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'EState_VSA10', 'EState_VSA11', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'VSA_EState10', 'NumAromaticCarbocycles', 'NumAromaticHeterocycles', 'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumAliphaticCarbocycles', 'NumAliphaticHeterocycles']).union(required_descriptors)),\n",
    "    \n",
    "    'Rg': list(set(['MolWt', 'ExactMolWt', 'HeavyAtomCount', 'NumRotatableBonds', 'FractionCsp3', 'FractionCsp2', 'FractionCsp', 'LabuteASA', 'TPSA', 'MolLogP', 'MolMR', 'HallKierAlpha', 'Kappa1', 'Kappa2', 'Kappa3', 'Phi', 'Chi0', 'Chi1', 'Chi2', 'Chi3', 'Chi4', 'Chi0n', 'Chi1n', 'Chi2n', 'Chi3n', 'Chi4n', 'Chi0v', 'Chi1v', 'Chi2v', 'Chi3v', 'Chi4v', 'BalabanJ', 'BertzCT', 'Ipc', 'AvgIpc', 'MinPartialCharge', 'MaxPartialCharge', 'MinAbsPartialCharge', 'MaxAbsPartialCharge', 'MinEStateIndex', 'MaxEStateIndex', 'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', 'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'SMR_VSA1', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', 'SMR_VSA8', 'SlogP_VSA1', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'EState_VSA1', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'VSA_EState1', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'NumHBondDonors', 'NumHBondAcceptors', 'NumAromaticRings', 'NumSaturatedRings', 'NumAliphaticRings', 'RingCount', 'BCUT2D_MWHI', 'BCUT2D_MWLOW', 'BCUT2D_CHGHI', 'BCUT2D_CHGLO', 'BCUT2D_LOGPHI', 'BCUT2D_LOGPLOW', 'BCUT2D_MRHI', 'BCUT2D_MRLOW', 'fr_COO', 'fr_COO2', 'fr_C_O', 'fr_C_O_noCOO', 'fr_ester', 'fr_ether', 'fr_amide', 'fr_urea', 'fr_Ar_OH', 'fr_phenol', 'fr_benzene', 'fr_Ar_N', 'fr_NH0', 'fr_NH1', 'fr_NH2', 'fr_halogen', 'fr_Al_OH', 'fr_methoxy', 'fr_sulfide', 'fr_sulfone', 'NumHeteroatoms', 'FpDensityMorgan1', 'FpDensityMorgan2', 'FpDensityMorgan3', 'qed', 'NHOHCount', 'NOCount', 'fr_nitrile', 'fr_azide', 'fr_alkyne']).union(required_descriptors))\n",
    "}\n",
    "\n",
    "for key in filters:\n",
    "    filters[key] = list(set(filters[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0144c35b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:00:56.922844Z",
     "iopub.status.busy": "2025-09-15T19:00:56.922222Z",
     "iopub.status.idle": "2025-09-15T19:00:56.928285Z",
     "shell.execute_reply": "2025-09-15T19:00:56.927489Z"
    },
    "papermill": {
     "duration": 0.012187,
     "end_time": "2025-09-15T19:00:56.929410",
     "exception": false,
     "start_time": "2025-09-15T19:00:56.917223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def separate_subtables(train_df):\n",
    "    labels = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "    subtables = {}\n",
    "    for label in labels:\n",
    "        subtables[label] = train_df[['SMILES', label]][train_df[label].notna()]\n",
    "    return subtables\n",
    "\n",
    "def augment_smiles_dataset(smiles_list, labels, num_augments=3):\n",
    "    augmented_smiles = []\n",
    "    augmented_labels = []\n",
    "    for smiles, label in zip(smiles_list, labels):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        augmented_smiles.append(smiles)\n",
    "        augmented_labels.append(label)\n",
    "        for _ in range(num_augments):\n",
    "            rand_smiles = Chem.MolToSmiles(mol, doRandom=True)\n",
    "            augmented_smiles.append(rand_smiles)\n",
    "            augmented_labels.append(label)\n",
    "    return augmented_smiles, np.array(augmented_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a394eed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:00:56.939404Z",
     "iopub.status.busy": "2025-09-15T19:00:56.939163Z",
     "iopub.status.idle": "2025-09-15T19:11:16.109527Z",
     "shell.execute_reply": "2025-09-15T19:11:16.108615Z"
    },
    "papermill": {
     "duration": 619.177299,
     "end_time": "2025-09-15T19:11:16.111128",
     "exception": false,
     "start_time": "2025-09-15T19:00:56.933829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing label: Tg\n",
      "                                              SMILES     Tg\n",
      "0                         *CC(*)c1ccccc1C(=O)OCCCCCC   45.0\n",
      "4  *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...   34.0\n",
      "5               *OC(=O)CCCCCCCCC(=O)OC1COC2C(*)COC12    3.0\n",
      "7  *C(=O)Nc1ccc(Oc2ccc(Oc3ccc(NC(=O)c4ccc5c(c4)C(...  275.0\n",
      "8  *CC(*)(C)C(=O)OCCCCCCCCCOc1ccc2cc(C(=O)Oc3cccc...   18.0\n",
      "(8295, 2)\n",
      "After concat (with Mordred): (16590, 342)\n",
      "After variance cut: (16590, 303)\n",
      "Data stored for Tg with shape: (16590, 305)\n",
      "Test shape: (3, 303)\n",
      "Processing label: FFV\n",
      "                                              SMILES       FFV\n",
      "0                         *CC(*)c1ccccc1C(=O)OCCCCCC  0.374645\n",
      "1  *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...  0.370410\n",
      "2  *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...  0.378860\n",
      "3  *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)...  0.387324\n",
      "4  *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...  0.355470\n",
      "(7892, 2)\n",
      "After concat (with Mordred): (15784, 431)\n",
      "After variance cut: (15784, 377)\n",
      "Data stored for FFV with shape: (15784, 379)\n",
      "Test shape: (3, 377)\n",
      "Processing label: Tc\n",
      "                                               SMILES        Tc\n",
      "0                          *CC(*)c1ccccc1C(=O)OCCCCCC  0.205667\n",
      "10  *c1ccc(-c2ccc3c(c2)C(CCCCCCC#N)(CCCCCCC#N)c2cc...  0.487000\n",
      "11                     *CC(*)c1ccc(C(=O)O)c(C(=O)O)c1  0.171000\n",
      "31                            *CCCCCNC(=O)CCCCC(=O)N*  0.327000\n",
      "34           *CCCCCCCCCCCCCCCCCCNC(=O)NCCCCCCNC(=O)N*  0.383000\n",
      "(866, 2)\n",
      "After concat (with Mordred): (1732, 366)\n",
      "After variance cut: (1732, 301)\n",
      "Data stored for Tc with shape: (1732, 303)\n",
      "Test shape: (3, 301)\n",
      "Processing label: Density\n",
      "                                               SMILES   Density\n",
      "0                          *CC(*)c1ccccc1C(=O)OCCCCCC  0.932000\n",
      "10  *c1ccc(-c2ccc3c(c2)C(CCCCCCC#N)(CCCCCCC#N)c2cc...  0.901123\n",
      "11                     *CC(*)c1ccc(C(=O)O)c(C(=O)O)c1  1.184354\n",
      "37                                           *CC(*)CC  0.752000\n",
      "46                              *CC(*)C(=O)Oc1ccccc1C  1.061864\n",
      "(1297, 2)\n",
      "After concat (with Mordred): (2594, 432)\n",
      "After variance cut: (2594, 380)\n",
      "Data stored for Density with shape: (2594, 382)\n",
      "Test shape: (3, 380)\n",
      "Processing label: Rg\n",
      "                                               SMILES         Rg\n",
      "10  *c1ccc(-c2ccc3c(c2)C(CCCCCCC#N)(CCCCCCC#N)c2cc...  28.682441\n",
      "11                     *CC(*)c1ccc(C(=O)O)c(C(=O)O)c1  13.534248\n",
      "37                                           *CC(*)CC  13.872913\n",
      "46                              *CC(*)C(=O)Oc1ccccc1C  12.737463\n",
      "64                               *CC(*)C(=O)Oc1ccccc1  13.435339\n",
      "(614, 2)\n",
      "After concat (with Mordred): (1228, 411)\n",
      "After variance cut: (1228, 341)\n",
      "Data stored for Rg with shape: (1228, 343)\n",
      "Test shape: (3, 341)\n"
     ]
    }
   ],
   "source": [
    "from mordred import Calculator, descriptors\n",
    "\n",
    "def calculate_mordred_descriptors(mol):\n",
    "    \"\"\"Calculate Mordred descriptors for a molecule.\"\"\"\n",
    "    calc = Calculator(descriptors, ignore_3D=True)\n",
    "    result = calc(mol)\n",
    "    \n",
    "    mordred_dict = {}\n",
    "    for desc_name, desc_value in result.items():\n",
    "        try:\n",
    "            if isinstance(desc_value, (int, float)) and np.isfinite(desc_value):\n",
    "                mordred_dict[f'mordred_{desc_name}'] = desc_value\n",
    "            else:\n",
    "                mordred_dict[f'mordred_{desc_name}'] = None\n",
    "        except:\n",
    "            mordred_dict[f'mordred_{desc_name}'] = None\n",
    "    \n",
    "    return mordred_dict\n",
    "\n",
    "def smiles_to_combined_fingerprints_with_descriptors_batch(smiles_list, selected_descriptors, \n",
    "                                                           radius=2, n_bits=128, use_mordred=True):\n",
    "    \"\"\"\n",
    "    Enhanced version with optional Mordred descriptors\n",
    "    \"\"\"\n",
    "    generator = GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "    descriptor_functions = {name: func for name, func in Descriptors.descList if name in selected_descriptors}\n",
    "    \n",
    "    fingerprints = []\n",
    "    descriptors = []\n",
    "    valid_smiles = []\n",
    "    invalid_indices = []\n",
    "    \n",
    "    mol_cache = {}\n",
    "    for smiles in set(smiles_list):\n",
    "        mol_cache[smiles] = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    if use_mordred:\n",
    "        mordred_calc = Calculator(descriptors, ignore_3D=True)\n",
    "    \n",
    "    for i, smiles in enumerate(smiles_list):\n",
    "        mol = mol_cache.get(smiles)\n",
    "        if mol:\n",
    "            morgan_fp = generator.GetFingerprint(mol)\n",
    "            maccs_fp = MACCSkeys.GenMACCSKeys(mol)\n",
    "            combined_fp = np.concatenate([np.array(morgan_fp), np.array(maccs_fp)])\n",
    "            fingerprints.append(combined_fp)\n",
    "            \n",
    "            descriptor_values = {}\n",
    "            \n",
    "            for name, func in descriptor_functions.items():\n",
    "                try:\n",
    "                    descriptor_values[name] = func(mol)\n",
    "                except:\n",
    "                    descriptor_values[name] = None\n",
    "            \n",
    "            try:\n",
    "                descriptor_values['MolWt'] = Descriptors.MolWt(mol)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                descriptor_values['LogP'] = Descriptors.MolLogP(mol)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                descriptor_values['TPSA'] = rdMolDescriptors.CalcTPSA(mol)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                descriptor_values['RotatableBonds'] = Descriptors.NumRotatableBonds(mol)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                descriptor_values['NumAtoms'] = mol.GetNumAtoms()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                adj = rdmolops.GetAdjacencyMatrix(mol)\n",
    "                G = nx.from_numpy_array(adj)\n",
    "                if nx.is_connected(G):\n",
    "                    descriptor_values['graph_diameter'] = nx.diameter(G)\n",
    "                    descriptor_values['avg_shortest_path'] = nx.average_shortest_path_length(G)\n",
    "                else:\n",
    "                    descriptor_values['graph_diameter'] = 0\n",
    "                    descriptor_values['avg_shortest_path'] = 0\n",
    "                descriptor_values['num_cycles'] = len(list(nx.cycle_basis(G)))\n",
    "            except:\n",
    "                descriptor_values['graph_diameter'] = None\n",
    "                descriptor_values['avg_shortest_path'] = None\n",
    "                descriptor_values['num_cycles'] = None\n",
    "            \n",
    "            if use_mordred:\n",
    "                try:\n",
    "                    mordred_result = mordred_calc(mol)\n",
    "                    for desc_name, desc_value in mordred_result.items():\n",
    "                        try:\n",
    "                            if isinstance(desc_value, (int, float)) and np.isfinite(desc_value):\n",
    "                                descriptor_values[f'mordred_{desc_name}'] = desc_value\n",
    "                            else:\n",
    "                                descriptor_values[f'mordred_{desc_name}'] = None\n",
    "                        except:\n",
    "                            descriptor_values[f'mordred_{desc_name}'] = None\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            descriptor_values['SMILES'] = smiles\n",
    "            descriptors.append(descriptor_values)\n",
    "            valid_smiles.append(smiles)\n",
    "        else:\n",
    "            fingerprints.append(np.zeros(n_bits + 167))\n",
    "            descriptors.append(None)\n",
    "            valid_smiles.append(None)\n",
    "            invalid_indices.append(i)\n",
    "    \n",
    "    return np.array(fingerprints), descriptors, valid_smiles, invalid_indices\n",
    "\n",
    "mordred_important = [\n",
    "    'mordred_nBonds', 'mordred_nBondsO', 'mordred_nBondsS', 'mordred_nBondsD',\n",
    "    'mordred_nBondsT', 'mordred_nBondsA', 'mordred_nBondsM', 'mordred_nBondsKS',\n",
    "    'mordred_GATS1c', 'mordred_GATS2c', 'mordred_GATS3c', 'mordred_GATS4c',\n",
    "    'mordred_MATS1c', 'mordred_MATS2c', 'mordred_MATS3c', 'mordred_MATS4c',\n",
    "    'mordred_SpMax_D', 'mordred_SpMin_D', 'mordred_SpMax_A', 'mordred_SpMin_A',\n",
    "    'mordred_EState_VSA1', 'mordred_EState_VSA2', 'mordred_EState_VSA3',\n",
    "    'mordred_VSA_EState1', 'mordred_VSA_EState2', 'mordred_VSA_EState3',\n",
    "    'mordred_PEOE_VSA1', 'mordred_PEOE_VSA2', 'mordred_PEOE_VSA3',\n",
    "    'mordred_SMR_VSA1', 'mordred_SMR_VSA2', 'mordred_SMR_VSA3',\n",
    "    'mordred_SlogP_VSA1', 'mordred_SlogP_VSA2', 'mordred_SlogP_VSA3',\n",
    "    'mordred_MPC2', 'mordred_MPC3', 'mordred_MPC4', 'mordred_MPC5',\n",
    "    'mordred_AMID_C', 'mordred_AMID_N', 'mordred_AMID_O', 'mordred_AMID_X',\n",
    "    'mordred_TopoPSA', 'mordred_LabuteASA', 'mordred_TPSA',\n",
    "    'mordred_Diameter', 'mordred_Radius', 'mordred_TopoShapeIndex',\n",
    "    'mordred_PetitjeanIndex', 'mordred_GeomDiameter', 'mordred_GeomRadius'\n",
    "]\n",
    "\n",
    "for key in filters:\n",
    "    filters[key] = list(set(filters[key] + mordred_important))\n",
    "\n",
    "subtables = separate_subtables(train)\n",
    "test_smiles = test['SMILES'].tolist()\n",
    "test_ids = test['id'].values if 'id' in test.columns else range(len(test))\n",
    "\n",
    "TARGET_VARIABLES = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "labels = TARGET_VARIABLES\n",
    "\n",
    "output_df = pd.DataFrame({'id': test_ids})\n",
    "\n",
    "data_per_label = {}\n",
    "test_data_per_label = {}\n",
    "\n",
    "USE_MORDRED = True\n",
    "\n",
    "for label in labels:\n",
    "    print(f\"Processing label: {label}\")\n",
    "    print(subtables[label].head())\n",
    "    print(subtables[label].shape)\n",
    "    \n",
    "    original_smiles = subtables[label]['SMILES'].tolist()\n",
    "    original_labels = subtables[label][label].values\n",
    "    \n",
    "    original_smiles, original_labels = augment_smiles_dataset(original_smiles, original_labels, num_augments=1)\n",
    "    \n",
    "    fingerprints, descriptors, valid_smiles, invalid_indices = smiles_to_combined_fingerprints_with_descriptors_batch(\n",
    "        original_smiles, filters[label], radius=2, n_bits=128, use_mordred=USE_MORDRED)\n",
    "    \n",
    "    X = pd.DataFrame(descriptors)\n",
    "    y = np.delete(original_labels, invalid_indices)\n",
    "    \n",
    "    available_cols = [col for col in filters[label] if col in X.columns]\n",
    "    X = X[available_cols]\n",
    "    \n",
    "    fp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])\n",
    "    \n",
    "    fp_df.reset_index(drop=True, inplace=True)\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    X = pd.concat([X, fp_df], axis=1)\n",
    "    \n",
    "    training_columns = X.columns.tolist()\n",
    "    \n",
    "    print(f\"After concat (with Mordred): {X.shape}\")\n",
    "    \n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    threshold = 0.01\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    X_transformed = selector.fit_transform(X)\n",
    "    final_columns = selector.get_feature_names_out()\n",
    "    X = pd.DataFrame(X_transformed, columns=final_columns)\n",
    "    \n",
    "    print(f\"After variance cut: {X.shape}\")\n",
    "    \n",
    "    data_per_label[label] = pd.concat([X, pd.Series(y, name='Target'), pd.Series(valid_smiles, name='SMILES')], axis=1)\n",
    "    print(f\"Data stored for {label} with shape: {data_per_label[label].shape}\")\n",
    "    \n",
    "    fingerprints, descriptors, valid_smiles_test, invalid_indices = smiles_to_combined_fingerprints_with_descriptors_batch(\n",
    "        test_smiles, filters[label], radius=2, n_bits=128, use_mordred=USE_MORDRED)\n",
    "    \n",
    "    test_X = pd.DataFrame(descriptors)\n",
    "    available_test_cols = [col for col in available_cols if col in test_X.columns]\n",
    "    test_X = test_X[available_test_cols]\n",
    "    \n",
    "    fp_df = pd.DataFrame(fingerprints, columns=[f'FP_{i}' for i in range(fingerprints.shape[1])])\n",
    "    fp_df.reset_index(drop=True, inplace=True)\n",
    "    test_X.reset_index(drop=True, inplace=True)\n",
    "    test_X = pd.concat([test_X, fp_df], axis=1)\n",
    "    \n",
    "    test_X = test_X.reindex(columns=training_columns, fill_value=0)\n",
    "\n",
    "    test_X = test_X.replace([np.inf, -np.inf], np.nan)\n",
    "    test_X = test_X.fillna(0)\n",
    "\n",
    "    test_X_transformed = selector.transform(test_X)\n",
    "    test_X = pd.DataFrame(test_X_transformed, columns=final_columns)\n",
    "    \n",
    "    print(f\"Test shape: {test_X.shape}\")\n",
    "    test_data_per_label[label] = test_X\n",
    "\n",
    "for label in labels:\n",
    "    if 'Target' in data_per_label[label].columns:\n",
    "        data_per_label[label].rename(columns={'Target': label}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737dfc82",
   "metadata": {
    "papermill": {
     "duration": 0.00467,
     "end_time": "2025-09-15T19:11:16.121074",
     "exception": false,
     "start_time": "2025-09-15T19:11:16.116404",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf81dc11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:11:16.131814Z",
     "iopub.status.busy": "2025-09-15T19:11:16.131487Z",
     "iopub.status.idle": "2025-09-15T19:11:16.156532Z",
     "shell.execute_reply": "2025-09-15T19:11:16.155887Z"
    },
    "papermill": {
     "duration": 0.032154,
     "end_time": "2025-09-15T19:11:16.157849",
     "exception": false,
     "start_time": "2025-09-15T19:11:16.125695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ATOM_MAP = {\n",
    "    'C': 0, 'N': 1, 'O': 2, 'F': 3, 'P': 4, 'S': 5, 'Cl': 6, 'Br': 7, 'I': 8, 'H': 9,\n",
    "    'Si': 10, 'Na': 11, '*': 12, 'B': 13, 'Ge': 14, 'Sn': 15, 'Se': 16,\n",
    "    'Te': 17, 'Ca': 18, 'Cd': 19\n",
    "}\n",
    "ATOM_MAP_LEN = len(ATOM_MAP)\n",
    "\n",
    "LABEL_SPECIFIC_FEATURES = {\n",
    "    'Tg': [\"HallKierAlpha\", \"MolLogP\", \"NumRotatableBonds\", \"TPSA\"],\n",
    "    'FFV': [\"NHOHCount\", \"NumRotatableBonds\", \"MolWt\", \"TPSA\"],\n",
    "    'Tc': [\"MolLogP\", \"NumValenceElectrons\", \"SPS\", \"MolWt\"],\n",
    "    'Density': [\"MolWt\", \"MolMR\", \"FractionCSP3\", \"NumHeteroatoms\"],\n",
    "    'Rg': [\"HallKierAlpha\", \"MolWt\", \"NumValenceElectrons\", \"qed\"]\n",
    "}\n",
    "\n",
    "RDKIT_DESC_CALCULATORS = {name: func for name, func in Descriptors.descList}\n",
    "RDKIT_DESC_CALCULATORS['qed'] = Descriptors.qed\n",
    "\n",
    "def smiles_to_graph_label_specific(smiles_str: str, label: str, y_val=None):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles_str)\n",
    "        if mol is None: return None\n",
    "\n",
    "        global_features = []\n",
    "        features_to_calculate = LABEL_SPECIFIC_FEATURES.get(label, [])\n",
    "        for name in features_to_calculate:\n",
    "            calculator = RDKIT_DESC_CALCULATORS.get(name)\n",
    "            if calculator:\n",
    "                try:\n",
    "                    val = calculator(mol)\n",
    "                    global_features.append(val if np.isfinite(val) else 0.0)\n",
    "                except: global_features.append(0.0)\n",
    "            else: global_features.append(0.0)\n",
    "\n",
    "        node_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_f = [0] * ATOM_MAP_LEN\n",
    "            if atom.GetSymbol() in ATOM_MAP: atom_f[ATOM_MAP[atom.GetSymbol()]] = 1\n",
    "            atom_f.extend([\n",
    "                atom.GetAtomicNum(), atom.GetTotalDegree(), atom.GetFormalCharge(),\n",
    "                atom.GetTotalNumHs(), int(atom.GetIsAromatic())\n",
    "            ])\n",
    "            node_features.append(atom_f)\n",
    "        if not node_features: return None\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "        edge_indices, edge_attrs = [], []\n",
    "        for bond in mol.GetBonds():\n",
    "            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            edge_indices.extend([(i, j), (j, i)])\n",
    "            bond_type = bond.GetBondTypeAsDouble()\n",
    "            edge_attrs.extend([[bond_type], [bond_type]])\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attrs, dtype=torch.float) if edge_attrs else torch.empty((0, 1), dtype=torch.float)\n",
    "\n",
    "        data_obj = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data_obj.u = torch.tensor([global_features], dtype=torch.float)\n",
    "        if y_val is not None: data_obj.y = torch.tensor([[y_val]], dtype=torch.float)\n",
    "        return data_obj\n",
    "    except: return None\n",
    "\n",
    "def create_dynamic_mlp(input_dim, layer_list, dropout_list):\n",
    "    layers = []\n",
    "    current_dim = input_dim\n",
    "    for neurons, dropout in zip(layer_list, dropout_list):\n",
    "        layers.extend([torch.nn.Linear(current_dim, neurons), torch.nn.ReLU(), torch.nn.Dropout(dropout)])\n",
    "        current_dim = neurons\n",
    "    layers.append(torch.nn.Linear(current_dim, 1))\n",
    "    return torch.nn.Sequential(*layers)\n",
    "\n",
    "class TaskSpecificGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, num_global_features,\n",
    "                 hidden_channels_gnn, mlp_neurons, mlp_dropouts, heads=8):\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList([\n",
    "            GATConv(num_node_features, hidden_channels_gnn, heads=heads, edge_dim=num_edge_features),\n",
    "            GATConv(hidden_channels_gnn * heads, hidden_channels_gnn * 2, heads=heads, edge_dim=num_edge_features),\n",
    "            GATConv(hidden_channels_gnn * 2 * heads, hidden_channels_gnn * 4, heads=heads, concat=False, edge_dim=num_edge_features)\n",
    "        ])\n",
    "        combined_feature_size = (hidden_channels_gnn * 4) + num_global_features\n",
    "        self.readout_mlp = create_dynamic_mlp(combined_feature_size, mlp_neurons, mlp_dropouts)\n",
    "        self.config_args = {k: v for k, v in locals().items() if k not in ['self', '__class__']}\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, u, batch = data.x, data.edge_index, data.edge_attr, data.u, data.batch\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = F.relu(conv(x, edge_index, edge_attr))\n",
    "            if i < len(self.convs) - 1: x = F.dropout(x, p=0.5, training=self.training)\n",
    "        graph_embedding = global_mean_pool(x, batch)\n",
    "        combined_features = torch.cat([graph_embedding, u], dim=1)\n",
    "        return self.readout_mlp(combined_features)\n",
    "\n",
    "def load_gnn_model(label, model_dir):\n",
    "    model_path = os.path.join(model_dir, f\"gnn_model_{label}.pth\")\n",
    "    config_path = os.path.join(model_dir, f\"gnn_config_{label}.json\")\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    if not (os.path.exists(model_path) and os.path.exists(config_path)): return None\n",
    "    with open(config_path, 'r') as f: config = json.load(f)\n",
    "    try:\n",
    "        model = TaskSpecificGNN(**config).to(DEVICE)\n",
    "        model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        model.eval()\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model for {label}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scale_graph_features(data_list, u_scaler, x_scaler, atom_map_len):\n",
    "    for data in data_list:\n",
    "        data.u = torch.tensor(u_scaler.transform(data.u.numpy()), dtype=torch.float)\n",
    "        x_one_hot, x_continuous = data.x[:, :atom_map_len], data.x[:, atom_map_len:]\n",
    "        x_continuous_scaled = torch.tensor(x_scaler.transform(x_continuous.numpy()), dtype=torch.float)\n",
    "        data.x = torch.cat([x_one_hot, x_continuous_scaled], dim=1)\n",
    "    return data_list\n",
    "\n",
    "def predict_with_gnn(model, smiles_list, label, u_scaler, x_scaler, atom_map_len):\n",
    "    if model is None or u_scaler is None or x_scaler is None: return np.full(len(smiles_list), np.nan)\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    test_data_raw = [smiles_to_graph_label_specific(s, label) for s in smiles_list]\n",
    "    valid_indices = [i for i, d in enumerate(test_data_raw) if d is not None]\n",
    "    valid_data = [d for d in test_data_raw if d is not None]\n",
    "    if not valid_data: return np.full(len(smiles_list), np.nan)\n",
    "    \n",
    "    valid_data_scaled = scale_graph_features(valid_data, u_scaler, x_scaler, atom_map_len)\n",
    "    loader = PyGDataLoader(valid_data_scaled, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            preds.append(model(data.to(DEVICE)).cpu())\n",
    "    \n",
    "    scaled_preds = torch.cat(preds, dim=0).numpy().flatten()\n",
    "    final_preds = np.full(len(smiles_list), np.nan)\n",
    "    if len(scaled_preds) == len(valid_indices): final_preds[valid_indices] = scaled_preds\n",
    "    return final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c676dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:11:16.169541Z",
     "iopub.status.busy": "2025-09-15T19:11:16.168857Z",
     "iopub.status.idle": "2025-09-15T19:11:40.500494Z",
     "shell.execute_reply": "2025-09-15T19:11:40.499391Z"
    },
    "papermill": {
     "duration": 24.338775,
     "end_time": "2025-09-15T19:11:40.501872",
     "exception": false,
     "start_time": "2025-09-15T19:11:16.163097",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n==================== Processing GNN for label: Tg ====================\n",
      "Loaded 10 models for Tg ensemble.\n",
      "\\n==================== Processing GNN for label: FFV ====================\n",
      "Loaded 10 models for FFV ensemble.\n",
      "\\n==================== Processing GNN for label: Tc ====================\n",
      "Loaded 10 models for Tc ensemble.\n",
      "\\n==================== Processing GNN for label: Density ====================\n",
      "Loaded 10 models for Density ensemble.\n",
      "\\n==================== Processing GNN for label: Rg ====================\n",
      "Loaded 10 models for Rg ensemble.\n",
      "\\nGNN Predictions Complete:\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  189.751849  0.373679  0.187229  1.168659  21.798934\n",
      "1  1422188626  174.595008  0.375173  0.266134  1.103980  23.134463\n",
      "2  2032016830  111.029738  0.350787  0.250217  1.113623  19.598631\n"
     ]
    }
   ],
   "source": [
    "def run_gnn_predictions(test_smiles_list, test_ids_list):\n",
    "    \"\"\"Loads all pre-trained GNN models and scalers to generate predictions for the test set.\"\"\"\n",
    "    gnn_preds_df = pd.DataFrame({'id': test_ids_list})\n",
    "    model_base_dir = '/kaggle/input/neurips-2025/GATConv_v29/models/gnn/'\n",
    "    \n",
    "    mlp_configs = {\n",
    "        \"Tg\": {\"neurons\": [512, 256, 128], \"dropouts\": [0.5, 0.4, 0.2]},\n",
    "        \"Density\": {\"neurons\": [1024, 256, 64], \"dropouts\": [0.5, 0.4, 0.3]},\n",
    "        \"FFV\": {\"neurons\": [1024, 512, 64], \"dropouts\": [0.6, 0.5, 0.4]},\n",
    "        \"Tc\": {\"neurons\": [128, 64], \"dropouts\": [0.4, 0.3]},\n",
    "        \"Rg\": {\"neurons\": [128, 64, 64], \"dropouts\": [0.4, 0.3, 0.3]},\n",
    "    }\n",
    "\n",
    "    for label in TARGET_VARIABLES:\n",
    "        print(f\"\\\\n{'='*20} Processing GNN for label: {label} {'='*20}\")\n",
    "        try:\n",
    "            y_scaler = joblib.load(f'{model_base_dir}gnn_yscaler_{label}.joblib')\n",
    "            u_scaler = joblib.load(f'{model_base_dir}gnn_uscaler_{label}.joblib')\n",
    "            x_scaler = joblib.load(f'{model_base_dir}gnn_xscaler_{label}.joblib')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"CRITICAL: Scaler files not found for {label}. Cannot make predictions.\")\n",
    "            gnn_preds_df[label] = 0.0 # Fallback\n",
    "            continue\n",
    "\n",
    "        ensemble_models = []\n",
    "        for fold in range(10): # There are 10 pre-trained models per target\n",
    "            model = load_gnn_model(f\"{label}_fold{fold}\", model_base_dir)\n",
    "            if model: ensemble_models.append(model)\n",
    "        \n",
    "        if not ensemble_models:\n",
    "            print(f\"CRITICAL: No models loaded for {label}.\")\n",
    "            gnn_preds_df[label] = y_scaler.inverse_transform([[0.0]])[0][0] # Fallback to median\n",
    "            continue\n",
    "\n",
    "        print(f\"Loaded {len(ensemble_models)} models for {label} ensemble.\")\n",
    "        \n",
    "        all_fold_preds_scaled = [\n",
    "            predict_with_gnn(model, test_smiles_list, label, u_scaler, x_scaler, ATOM_MAP_LEN)\n",
    "            for model in ensemble_models\n",
    "        ]\n",
    "        \n",
    "        final_preds_scaled = np.nanmean(np.stack(all_fold_preds_scaled), axis=0)\n",
    "        filled_preds_scaled = pd.Series(final_preds_scaled).fillna(0.0) # Impute with scaled median (0.0 for RobustScaler)\n",
    "        \n",
    "        final_preds_original = y_scaler.inverse_transform(filled_preds_scaled.values.reshape(-1, 1)).flatten()\n",
    "        gnn_preds_df[label] = final_preds_original\n",
    "\n",
    "    return gnn_preds_df\n",
    "\n",
    "gnn_predictions_df = run_gnn_predictions(test_smiles, test_ids)\n",
    "print(\"\\\\nGNN Predictions Complete:\")\n",
    "print(gnn_predictions_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44992f72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:11:40.514678Z",
     "iopub.status.busy": "2025-09-15T19:11:40.514373Z",
     "iopub.status.idle": "2025-09-15T19:18:11.531697Z",
     "shell.execute_reply": "2025-09-15T19:18:11.530811Z"
    },
    "papermill": {
     "duration": 391.025231,
     "end_time": "2025-09-15T19:18:11.533083",
     "exception": false,
     "start_time": "2025-09-15T19:11:40.507852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for Tg...\n",
      "Data shape: (16590, 303), features: 303\n",
      "  Fold 1/5\n",
      "    Fold 1 MAE: 15.0686\n",
      "  Fold 2/5\n",
      "    Fold 2 MAE: 14.8264\n",
      "  Fold 3/5\n",
      "    Fold 3 MAE: 14.4393\n",
      "  Fold 4/5\n",
      "    Fold 4 MAE: 14.9533\n",
      "  Fold 5/5\n",
      "    Fold 5 MAE: 15.1358\n",
      "  CV MAE for Tg: 14.8847 ± 0.2463\n",
      "\n",
      "Training for FFV...\n",
      "Data shape: (15784, 377), features: 377\n",
      "  Fold 1/5\n",
      "    Fold 1 MAE: 0.0054\n",
      "  Fold 2/5\n",
      "    Fold 2 MAE: 0.0054\n",
      "  Fold 3/5\n",
      "    Fold 3 MAE: 0.0057\n",
      "  Fold 4/5\n",
      "    Fold 4 MAE: 0.0055\n",
      "  Fold 5/5\n",
      "    Fold 5 MAE: 0.0055\n",
      "  CV MAE for FFV: 0.0055 ± 0.0001\n",
      "\n",
      "Training for Tc...\n",
      "Data shape: (1732, 301), features: 301\n",
      "  Fold 1/5\n",
      "    Fold 1 MAE: 0.0179\n",
      "  Fold 2/5\n",
      "    Fold 2 MAE: 0.0162\n",
      "  Fold 3/5\n",
      "    Fold 3 MAE: 0.0183\n",
      "  Fold 4/5\n",
      "    Fold 4 MAE: 0.0180\n",
      "  Fold 5/5\n",
      "    Fold 5 MAE: 0.0201\n",
      "  CV MAE for Tc: 0.0181 ± 0.0012\n",
      "\n",
      "Training for Density...\n",
      "Data shape: (2594, 380), features: 380\n",
      "  Fold 1/5\n",
      "    Fold 1 MAE: 0.0178\n",
      "  Fold 2/5\n",
      "    Fold 2 MAE: 0.0185\n",
      "  Fold 3/5\n",
      "    Fold 3 MAE: 0.0161\n",
      "  Fold 4/5\n",
      "    Fold 4 MAE: 0.0187\n",
      "  Fold 5/5\n",
      "    Fold 5 MAE: 0.0210\n",
      "  CV MAE for Density: 0.0184 ± 0.0016\n",
      "\n",
      "Training for Rg...\n",
      "Data shape: (1228, 341), features: 341\n",
      "  Fold 1/5\n",
      "    Fold 1 MAE: 0.5655\n",
      "  Fold 2/5\n",
      "    Fold 2 MAE: 0.7134\n",
      "  Fold 3/5\n",
      "    Fold 3 MAE: 0.6564\n",
      "  Fold 4/5\n",
      "    Fold 4 MAE: 0.6925\n",
      "  Fold 5/5\n",
      "    Fold 5 MAE: 0.7718\n",
      "  CV MAE for Rg: 0.6798 ± 0.0684\n",
      "\n",
      "=== Cross-validation Results Summary ===\n",
      "Tg: 14.8847\n",
      "FFV: 0.0055\n",
      "Tc: 0.0181\n",
      "Density: 0.0184\n",
      "Rg: 0.6798\n"
     ]
    }
   ],
   "source": [
    "TARGET_VARIABLES = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "RANDOM_STATE = 42\n",
    "N_FOLDS = 5\n",
    "\n",
    "predictions_df = pd.DataFrame({'id': test_ids})\n",
    "mae_scores = {}\n",
    "oof_data = {}\n",
    "\n",
    "for target in TARGET_VARIABLES:\n",
    "    print(f\"\\nTraining for {target}...\")\n",
    "\n",
    "    X_test = test_data_per_label[target]\n",
    "    \n",
    "    full_data = data_per_label[target].dropna(subset=[target])\n",
    "    y = full_data[target]\n",
    "    X = full_data.drop(columns=[target, 'SMILES'])\n",
    "    smiles_for_oof = full_data['SMILES']\n",
    "\n",
    "    X_features = X.to_numpy()\n",
    "    y_values = y.to_numpy()\n",
    "    X_test_array = X_test.to_numpy()\n",
    "    \n",
    "    X_features = np.nan_to_num(X_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_test_array = np.nan_to_num(X_test_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    X_features = np.clip(X_features, -1e10, 1e10)\n",
    "    X_test_array = np.clip(X_test_array, -1e10, 1e10)\n",
    "    \n",
    "    print(f\"Data shape: {X_features.shape}, features: {X.shape[1]}\")\n",
    "    \n",
    "    kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    oof_predictions = np.zeros(len(y_values))\n",
    "    test_predictions = np.zeros((N_FOLDS, len(X_test_array)))\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_features)):\n",
    "        print(f\"  Fold {fold + 1}/{N_FOLDS}\")\n",
    "        \n",
    "        X_train_fold, X_val_fold = X_features[train_idx], X_features[val_idx]\n",
    "        y_train_fold, y_val_fold = y_values[train_idx], y_values[val_idx]\n",
    "        \n",
    "        if target == \"Tg\":\n",
    "            xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.1, max_depth=8, reg_lambda=2.3, subsample=0.7, colsample_bytree=0.7, random_state=RANDOM_STATE, tree_method='hist', early_stopping_rounds=50)\n",
    "            rf_model = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_leaf=2, max_features=0.7, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        elif target == \"Rg\":\n",
    "            xgb_model = XGBRegressor(n_estimators=300, learning_rate=0.08, max_depth=5, reg_lambda=1.0, random_state=RANDOM_STATE, tree_method='hist', early_stopping_rounds=30)\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        elif target == \"FFV\":\n",
    "            xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.08, max_depth=4, reg_lambda=3.0, random_state=RANDOM_STATE, tree_method='hist', early_stopping_rounds=50)\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        elif target == \"Tc\":\n",
    "            xgb_model = XGBRegressor(n_estimators=400, learning_rate=0.05, max_depth=5, reg_lambda=10.0, random_state=RANDOM_STATE, tree_method='hist', early_stopping_rounds=40)\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "        elif target == \"Density\":\n",
    "            xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.1, max_depth=5, reg_lambda=3.0, random_state=RANDOM_STATE, tree_method='hist', early_stopping_rounds=50)\n",
    "            rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "        xgb_model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], verbose=False)\n",
    "        xgb_oof_preds = xgb_model.predict(X_val_fold)\n",
    "        xgb_test_preds = xgb_model.predict(X_test_array)\n",
    "        \n",
    "        rf_model.fit(X_train_fold, y_train_fold)\n",
    "        rf_oof_preds = rf_model.predict(X_val_fold)\n",
    "        rf_test_preds = rf_model.predict(X_test_array)\n",
    "        \n",
    "        fold_oof_preds = (xgb_oof_preds + rf_oof_preds) / 2\n",
    "        fold_test_preds = (xgb_test_preds + rf_test_preds) / 2\n",
    "        \n",
    "        oof_predictions[val_idx] = fold_oof_preds\n",
    "        test_predictions[fold] = fold_test_preds\n",
    "        \n",
    "        fold_mae = mean_absolute_error(y_val_fold, fold_oof_preds)\n",
    "        fold_scores.append(fold_mae)\n",
    "        print(f\"    Fold {fold + 1} MAE: {fold_mae:.4f}\")\n",
    "    \n",
    "    cv_mae = mean_absolute_error(y_values, oof_predictions)\n",
    "    mae_scores[target] = cv_mae\n",
    "    print(f\"  CV MAE for {target}: {cv_mae:.4f} ± {np.std(fold_scores):.4f}\")\n",
    "    \n",
    "    predictions_df[target] = np.mean(test_predictions, axis=0)\n",
    "    \n",
    "    oof_data[target] = {\n",
    "        'y_true': y_values, \n",
    "        'y_pred_xgb_rf': oof_predictions,\n",
    "        'smiles': smiles_for_oof.values \n",
    "    }\n",
    "\n",
    "print(\"\\n=== Cross-validation Results Summary ===\")\n",
    "for target, score in mae_scores.items():\n",
    "    print(f\"{target}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0814b0b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T19:18:11.549950Z",
     "iopub.status.busy": "2025-09-15T19:18:11.549640Z",
     "iopub.status.idle": "2025-09-15T20:24:49.452290Z",
     "shell.execute_reply": "2025-09-15T20:24:49.451434Z"
    },
    "papermill": {
     "duration": 3997.919676,
     "end_time": "2025-09-15T20:24:49.461615",
     "exception": false,
     "start_time": "2025-09-15T19:18:11.541939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating GNN predictions on TEST data ---\n",
      "\\n==================== Processing GNN for label: Tg ====================\n",
      "Loaded 10 models for Tg ensemble.\n",
      "\\n==================== Processing GNN for label: FFV ====================\n",
      "Loaded 10 models for FFV ensemble.\n",
      "\\n==================== Processing GNN for label: Tc ====================\n",
      "Loaded 10 models for Tc ensemble.\n",
      "\\n==================== Processing GNN for label: Density ====================\n",
      "Loaded 10 models for Density ensemble.\n",
      "\\n==================== Processing GNN for label: Rg ====================\n",
      "Loaded 10 models for Rg ensemble.\n",
      "\n",
      " GNN Test Predictions Complete:\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  189.751847  0.373679  0.187229  1.168659  21.798934\n",
      "1  1422188626  174.595003  0.375173  0.266134  1.103980  23.134463\n",
      "2  2032016830  111.029739  0.350787  0.250217  1.113623  19.598631\n",
      "\n",
      "--- Generating GNN predictions on ALL TRAINING data ---\n",
      "Found 30132 unique training SMILES to predict.\n",
      "\\n==================== Processing GNN for label: Tg ====================\n",
      "Loaded 10 models for Tg ensemble.\n",
      "\\n==================== Processing GNN for label: FFV ====================\n",
      "Loaded 10 models for FFV ensemble.\n",
      "\\n==================== Processing GNN for label: Tc ====================\n",
      "Loaded 10 models for Tc ensemble.\n",
      "\\n==================== Processing GNN for label: Density ====================\n",
      "Loaded 10 models for Density ensemble.\n",
      "\\n==================== Processing GNN for label: Rg ====================\n",
      "Loaded 10 models for Rg ensemble.\n",
      "\n",
      " GNN Training Predictions Complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Generating GNN predictions on TEST data ---\")\n",
    "gnn_test_predictions_df = run_gnn_predictions(test_smiles, test_ids)\n",
    "print(\"\\n GNN Test Predictions Complete:\")\n",
    "print(gnn_test_predictions_df.head())\n",
    "print(\"\\n--- Generating GNN predictions on ALL TRAINING data ---\")\n",
    "all_training_smiles = set()\n",
    "for target in TARGET_VARIABLES:\n",
    "    all_training_smiles.update(oof_data[target]['smiles'])\n",
    "\n",
    "unique_train_smiles_list = list(all_training_smiles)\n",
    "unique_train_ids = list(range(len(unique_train_smiles_list)))\n",
    "print(f\"Found {len(unique_train_smiles_list)} unique training SMILES to predict.\")\n",
    "\n",
    "gnn_train_preds_df = run_gnn_predictions(unique_train_smiles_list, unique_train_ids)\n",
    "gnn_train_preds_df['SMILES'] = unique_train_smiles_list\n",
    "\n",
    "print(\"\\n GNN Training Predictions Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317e443",
   "metadata": {
    "papermill": {
     "duration": 0.011668,
     "end_time": "2025-09-15T20:24:49.481543",
     "exception": false,
     "start_time": "2025-09-15T20:24:49.469875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Meta-model training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97a7df0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-15T20:24:49.509509Z",
     "iopub.status.busy": "2025-09-15T20:24:49.509061Z",
     "iopub.status.idle": "2025-09-15T20:24:49.842170Z",
     "shell.execute_reply": "2025-09-15T20:24:49.841303Z"
    },
    "papermill": {
     "duration": 0.350488,
     "end_time": "2025-09-15T20:24:49.843634",
     "exception": false,
     "start_time": "2025-09-15T20:24:49.493146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stacking models for Tg ---\n",
      "  Meta-Model MAE on OOF data for Tg: 13.8022\n",
      "--- Stacking models for FFV ---\n",
      "  Meta-Model MAE on OOF data for FFV: 0.0027\n",
      "--- Stacking models for Tc ---\n",
      "  Meta-Model MAE on OOF data for Tc: 0.0148\n",
      "--- Stacking models for Density ---\n",
      "  Meta-Model MAE on OOF data for Density: 0.0159\n",
      "--- Stacking models for Rg ---\n",
      "  Meta-Model MAE on OOF data for Rg: 0.4977\n",
      "\n",
      "=== Meta-Model OOF MAE Summary ===\n",
      "         Meta_Model_MAE\n",
      "Tg            13.802226\n",
      "FFV            0.002717\n",
      "Tc             0.014811\n",
      "Density        0.015938\n",
      "Rg             0.497692\n",
      "\n",
      "Final ensembled submission saved to submission.csv\n",
      "\n",
      "--- Final Submission Preview ---\n",
      "           id          Tg       FFV        Tc   Density         Rg\n",
      "0  1109053969  164.702530  0.374093  0.181062  1.164261  20.206488\n",
      "1  1422188626  154.549393  0.375053  0.268137  1.112596  19.997940\n",
      "2  2032016830  143.726624  0.350086  0.241577  1.112596  19.832661\n"
     ]
    }
   ],
   "source": [
    "final_predictions_df = pd.DataFrame({'id': test_ids})\n",
    "meta_model_scores = {}\n",
    "\n",
    "gnn_train_preds_lookup = gnn_train_preds_df.set_index('SMILES')\n",
    "\n",
    "for target in TARGET_VARIABLES:\n",
    "    print(f\"--- Stacking models for {target} ---\")\n",
    "    \n",
    "\n",
    "    oof_df = pd.DataFrame({\n",
    "        'SMILES': oof_data[target]['smiles'],\n",
    "        'y_true': oof_data[target]['y_true'],\n",
    "        'y_pred_xgb_rf': oof_data[target]['y_pred_xgb_rf']\n",
    "    })\n",
    "    \n",
    "\n",
    "    oof_df = oof_df.merge(gnn_train_preds_lookup[[target]], on='SMILES', how='left')\n",
    "    oof_df.rename(columns={target: 'y_pred_gnn'}, inplace=True)\n",
    "    \n",
    "\n",
    "    oof_aligned = oof_df.dropna()\n",
    "\n",
    "    X_meta_train = oof_aligned[['y_pred_xgb_rf', 'y_pred_gnn']].values\n",
    "    y_meta_train = oof_aligned['y_true'].values\n",
    "\n",
    "\n",
    "    meta_model = XGBRegressor(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=RANDOM_STATE)\n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "    \n",
    "\n",
    "    meta_oof_preds = meta_model.predict(X_meta_train)\n",
    "    meta_mae = mean_absolute_error(y_meta_train, meta_oof_preds)\n",
    "    meta_model_scores[target] = meta_mae\n",
    "    print(f\"  Meta-Model MAE on OOF data for {target}: {meta_mae:.4f}\")\n",
    "    \n",
    "\n",
    "    test_pred_xgb_rf = predictions_df[target].values\n",
    "    test_pred_gnn = gnn_test_predictions_df[target].values\n",
    "    X_meta_test = np.vstack([test_pred_xgb_rf, test_pred_gnn]).T\n",
    "    \n",
    "\n",
    "    final_predictions_df[target] = meta_model.predict(X_meta_test)\n",
    "\n",
    "\n",
    "print(\"\\n=== Meta-Model OOF MAE Summary ===\")\n",
    "print(pd.DataFrame.from_dict(meta_model_scores, orient='index', columns=['Meta_Model_MAE']))\n",
    "\n",
    "final_predictions_df.to_csv('submission.csv', index=False)\n",
    "print(\"\\nFinal ensembled submission saved to submission.csv\")\n",
    "\n",
    "print(\"\\n--- Final Submission Preview ---\")\n",
    "print(final_predictions_df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12966160,
     "isSourceIdPinned": false,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 5121438,
     "sourceId": 8566505,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7279248,
     "sourceId": 11605551,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7678100,
     "sourceId": 12189904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7690162,
     "sourceId": 12207625,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7706066,
     "sourceId": 12237259,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7714586,
     "sourceId": 12243815,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7787167,
     "sourceId": 12351859,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7958862,
     "sourceId": 12600616,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8114882,
     "sourceId": 12831234,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8142103,
     "sourceId": 12871401,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8238327,
     "sourceId": 13048231,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5098.657877,
   "end_time": "2025-09-15T20:24:53.034870",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-15T18:59:54.376993",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
